{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter_tunning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lJGRsJSp6Iq1",
        "SDEeIkyL5h_G"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuGweZcNS_eq",
        "colab_type": "text"
      },
      "source": [
        "This notebook involves the code for hyperparameter tuning for the standard one class models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eHU3KGY6UN7",
        "colab_type": "text"
      },
      "source": [
        "# Importing the datasets and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aNO0R_5G8YxZ",
        "colab": {}
      },
      "source": [
        "#Install the pyod library\n",
        "!git clone https://github.com/yzhao062/pyod.git\n",
        "%cd pyod\n",
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsAzI6AsObgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the main libraries\n",
        "from sklearn import datasets, metrics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from scipy.spatial.distance import squareform\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from numpy import percentile\n",
        "import warnings\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "import seaborn as sns\n",
        "from random import Random\n",
        "\n",
        "# Import all models\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.iforest import IForest\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.cblof import CBLOF\n",
        "from sklearn.mixture import GaussianMixture"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQowKgk6ISZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOrfH6VIpN6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=  pd.read_csv('/content/drive/My Drive/cocrystal_design-master/data_test/df_reduced.csv')\n",
        "df=df.fillna(0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeEnuMs3vmTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contamination = 0.05\n",
        "class GMM(GaussianMixture):\n",
        "  def __init__(self, n_components, covariance_type, random_state):\n",
        "    super().__init__(n_components=n_components , covariance_type=covariance_type, random_state=random_state)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    super().fit(X, y)\n",
        "    self.prob = super().score_samples(X)\n",
        "    self.c = percentile(self.prob, 100 * contamination)\n",
        "\n",
        "  def predict(self, X):\n",
        "    scores = []\n",
        "    proba=super().score_samples(X)\n",
        "    \n",
        "    scores =(proba <= self.c).astype('int').ravel()\n",
        "\n",
        "    return scores"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfHT1mLkKSv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train=df.iloc[:1722, 1:].values\n",
        "shuffle(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33YOEew4Js1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1025dd20-55fb-43c1-9eb0-7b5fa0d3d5fa"
      },
      "source": [
        "#Find GMM hyperparameters\n",
        "\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "# Create the dataset\n",
        "\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  n_components, covariance_type = int(params['n_components']), str(params['covariance_type'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  GMM(n_components= n_components, covariance_type=covariance_type, random_state=0,)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data, np.zeros(X_train.shape[0]))\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= {'n_components':hp.quniform('n_components', 1, 5, 1),\n",
        "        'covariance_type': hp.choice('covariance_type', [ 'tied' ,'full','spherical','diag'])}\n",
        "  \n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "gmm_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print(gmm_best)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:04<00:00, 10.66it/s, best loss: 0.19769602378298012]\n",
            "{'covariance_type': 0, 'n_components': 4.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLr6TihuirEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a4dfc685-9cd8-4498-ec5d-bb9fccd17419"
      },
      "source": [
        "# Hyperparameters for HBOS algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  n_bins, alpha, tol = int(params['n_bins']), float(params['alpha']), float(params['tol'])\n",
        "  #n_components,  covariance_type= int(params['n_components']), str(params['covariance_type']) #int(params['n_clusters']) , int(params['beta']), (params['alpha'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model = HBOS(contamination=0.05, n_bins= n_bins, alpha=alpha, tol=tol)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= {'n_bins': hp.quniform('n_bins', 10, 20, 1), \n",
        "   'alpha': hp.quniform('alpha', 0.1, 0.9, 0.1),\n",
        "    'tol': hp.quniform('tol', 0.1, 0.5, 0.1) }\n",
        "\n",
        "\n",
        "# HBOS(contamination=0.05, n_bins=15, alpha=0.6), \n",
        "\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "hbos_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best HBOS hyperparemeters:', hbos_best)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:01<00:00, 32.03it/s, best loss: 0.4346014492753624]\n",
            "Best HBOS hyperparemeters: {'alpha': 0.6000000000000001, 'n_bins': 15.0, 'tol': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNVsuxHrp4Tt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dd885cd2-795e-4032-c509-22c9a32e9f95"
      },
      "source": [
        "# Hyperparameters for OCSVM algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  nu, degree, gamma=  float(params['nu']) , int(params['degree']),float(params['gamma'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  OCSVM(contamination=0.05, kernel='rbf' , nu= nu, degree=degree, gamma=gamma)\n",
        "\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= {'nu': hp.quniform('nu', 0.1, 0.9, 0.1),\n",
        "        'degree': hp.quniform('degree', 2, 10, 1) , \n",
        "        'gamma' : hp.quniform('gamma', 1, 10, 1)}\n",
        " \n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "ocsvm_best= fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best OCSVM hyperparemeters', ocsvm_best)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:03<00:00,  1.28s/it, best loss: 0.15652173913043477]\n",
            "Best OCSVM hyperparemeters {'degree': 10.0, 'gamma': 7.0, 'nu': 0.5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFJs4kkQp5CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "24bef81c-6c4b-4e80-b227-3111b7e8cf8b"
      },
      "source": [
        "# Hyperparameters for Feature Bagging algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  n_neighbors = int(params['n_neighbors'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model = FeatureBagging(LOF(n_neighbors=n_neighbors), contamination=0.05, random_state=0)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= {'n_neighbors':hp.quniform('n_neighbors', 1, 15, 1)}\n",
        "\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "featbag_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best Feature Bagging hyperparemeters:' , featbag_best)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:49<00:00,  3.39s/it, best loss: 0.08695652173913049]\n",
            "Best Feature Bagging hyperparemeters: {'n_neighbors': 8.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-WE61A7p5wG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "273a1830-b4f1-4519-e91f-b3817bfd1c11"
      },
      "source": [
        "# Hyperparameters for kNN algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  method, n_neighbors = str(params['methods']) , int(params['n_neighbors'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  model =  KNN(contamination=0.05, method=method, n_neighbors=n_neighbors) \n",
        "  LOF(n_neighbors=n_neighbors, contamination=0.05)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= { 'methods': hp.choice('methods', ['largest' ,'median', 'mean']),\n",
        "    'n_neighbors':hp.quniform('n_neighbors', 5, 25, 1)}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "knn_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best kNN hyperparemeters:', knn_best)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:47<00:00,  1.05it/s, best loss: 0.08695652173913049]\n",
            "Best kNN hyperparemeters: {'methods': 2, 'n_neighbors': 15.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t72pCydLp6YF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0e78a416-724a-4229-cfed-78ab9afea462"
      },
      "source": [
        "# Hyperparameters for CBLOF algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  n_clusters,alpha, beta = int(params['n_clusters']), float(params['alpha']), int(params['beta'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model = CBLOF(contamination=0.05, n_clusters=n_clusters, alpha=alpha, beta=beta, random_state=0)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= {'n_clusters':hp.quniform('n_clusters', 5, 15, 1),\n",
        "    'alpha': hp.quniform('alpha', 0.2, 0.9, 0.1),\n",
        "    'beta': hp.quniform('beta', 2, 10, 2)  }\n",
        "\n",
        "#CBLOF(contamination=0.05,  alpha=0.9, beta=4, n_clusters=12\n",
        "\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "cblof_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best CBLOF hyperparemeters:', cblof_best)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:20<00:00,  2.47it/s, best loss: 0.12454710144927539]\n",
            "Best CBLOF hyperparemeters: {'alpha': 0.2, 'beta': 4.0, 'n_clusters': 6.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir7FB0YkwoXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "20c6b64a-4890-480f-8c43-f490a4584bd6"
      },
      "source": [
        "# Hyperparameters for LOF algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  n_neighbors = int(params['n_neighbors'])\n",
        "  #n_bins, alpha, tol = int(params['n_bins']), float(params['alpha']), float(params['tol'])\n",
        "  #n_components,  covariance_type= int(params['n_components']), str(params['covariance_type']) #int(params['n_clusters']) , int(params['beta']), (params['alpha'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  LOF(n_neighbors=n_neighbors, contamination=0.05)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= {'n_neighbors':hp.quniform('n_neighbors', 5, 25, 1)}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0) \n",
        "# Print the hyperparameters that minimize the loss function\n",
        "lof_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate )\n",
        "print('Best LOF hyperparemeters:',lof_best)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:28<00:00,  1.72it/s, best loss: 0.061352657004830946]\n",
            "Best LOF hyperparemeters: {'n_neighbors': 10.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNGbX1i8z9Wd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ff1e7a29-4a6e-4329-cc3a-87d046a20e04"
      },
      "source": [
        "# Hyperparameters for Iforest algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp\n",
        "\n",
        "# Perform k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  n_estimators = int(params['n_estimators'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  IForest(behaviour=\"new\", bootstrap=False, contamination=0.05, n_estimators=n_estimators,  max_features=1.0, max_samples=1000)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= {'n_estimators':hp.quniform('n_estimators', 100, 500, 10)}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0) \n",
        "# Print the hyperparameters that minimize the loss function\n",
        "ifor_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate )\n",
        "print('Best Iforest hyperparemeters:', ifor_best)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:35<00:00,  1.92s/it, best loss: 0.221256038647343]\n",
            "Best Iforest hyperparemeters: {'n_estimators': 200.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy3AGK_CX8ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers = {\n",
        "    'Gaussiann Mixture Model (GMM)': GMM(n_components= gmm_best['n_components'], covariance_type=gmm_best['covariance_type'], random_state=0), \n",
        "    'K Nearest Neighbors (KNN)': KNN(contamination=0.05, method=knn_best['methods'], n_neighbors= knn_best['n_neighbors'] ),\n",
        "    'Histogram-base Outlier Detection (HBOS)': HBOS(contamination=0.05, n_bins=hbos_best['n_bins'], alpha=hbos_best['alpha']),\n",
        "    'Feature Bagging':\n",
        "     FeatureBagging(LOF(n_neighbors=featbag_best['n_neighbors']), contamination=0.05),\n",
        "    'Isolation Forest': IForest(behaviour=\"new\", bootstrap=False, contamination=0.05, n_estimators=ifor_best['n_estimators'],  max_features=1.0, max_samples=1000), \n",
        "    'One class SVM (OCSVM)': OCSVM(contamination=0.05, kernel='rbf' , nu=ocsvm_best['nu'] , degree=ocsvm_best['degree'], gamma=ocsvm_best['gamma']),\n",
        "    'Local Outlier Factor (LOF)':\n",
        "       LOF(n_neighbors=lof_best['n_neighbors'], contamination=0.05),\n",
        "     'CBLOF':   CBLOF(contamination=0.05,  alpha=cblof_best['alpha'], beta=cblof_best['beta'], n_clusters=cblof_best['n_clusters'])\n",
        "}"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQO4T3x4UeFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "97f160ce-4cb3-4bc5-a89a-9628e9e2c5c4"
      },
      "source": [
        "classifiers"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CBLOF': CBLOF(alpha=0.6000000000000001, beta=2.0, check_estimator=False,\n",
              "    clustering_estimator=None, contamination=0.05, n_clusters=6.0, n_jobs=1,\n",
              "    random_state=None, use_weights=False),\n",
              " 'Feature Bagging': FeatureBagging(base_estimator=LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
              "   metric_params=None, n_jobs=1, n_neighbors=11.0, p=2),\n",
              "         bootstrap_features=False, check_detector=True,\n",
              "         check_estimator=False, combination='average', contamination=0.05,\n",
              "         estimator_params={}, max_features=1.0, n_estimators=10, n_jobs=1,\n",
              "         random_state=None, verbose=0),\n",
              " 'Gaussiann Mixture Model (GMM)': GMM(covariance_type=2, n_components=3.0, random_state=0),\n",
              " 'Histogram-base Outlier Detection (HBOS)': HBOS(alpha=0.7000000000000001, contamination=0.05, n_bins=15.0, tol=0.5),\n",
              " 'Isolation Forest': IForest(behaviour='new', bootstrap=False, contamination=0.05,\n",
              "     max_features=1.0, max_samples=1000, n_estimators=200.0, n_jobs=1,\n",
              "     random_state=None, verbose=0),\n",
              " 'K Nearest Neighbors (KNN)': KNN(algorithm='auto', contamination=0.05, leaf_size=30, method=2,\n",
              "   metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=17.0, p=2,\n",
              "   radius=1.0),\n",
              " 'Local Outlier Factor (LOF)': LOF(algorithm='auto', contamination=0.05, leaf_size=30, metric='minkowski',\n",
              "   metric_params=None, n_jobs=1, n_neighbors=10.0, p=2),\n",
              " 'One class SVM (OCSVM)': OCSVM(cache_size=200, coef0=0.0, contamination=0.05, degree=10.0, gamma=7.0,\n",
              "    kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
              "    verbose=False)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULl-iFQGdeQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}