{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter_tunning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lJGRsJSp6Iq1",
        "SDEeIkyL5h_G"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuGweZcNS_eq",
        "colab_type": "text"
      },
      "source": [
        "This notebook involves the steps for hyperparameter tuning for the standard one class models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eHU3KGY6UN7",
        "colab_type": "text"
      },
      "source": [
        "# Importing the datasets and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aNO0R_5G8YxZ",
        "colab": {}
      },
      "source": [
        "#Install the pyod library\n",
        "!git clone https://github.com/yzhao062/pyod.git\n",
        "%cd pyod\n",
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsAzI6AsObgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "978edce3-45c5-4416-954e-60b2a4c48fd9"
      },
      "source": [
        "# Import the main libraries\n",
        "from sklearn import datasets, metrics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from scipy.spatial.distance import squareform\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from numpy import percentile\n",
        "import warnings\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "import seaborn as sns\n",
        "from random import Random\n",
        "\n",
        "# Import all models\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.iforest import IForest\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.cblof import CBLOF\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Hyperparameters for HBOS algorithm\n",
        "from hyperopt import STATUS_OK\n",
        "from sklearn.model_selection import KFold\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "from hyperopt import hp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQowKgk6ISZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "d0f28941-0224-4fea-dca9-1e8991e0fe98"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOrfH6VIpN6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df=  pd.read_csv('/content/drive/My Drive/cocrystal_design-master/data_test/df_reduced.csv')\n",
        "df= pd.read_pickle('/content/drive/My Drive/cocrystal_design-master/data_test/df_reduced.pkl')\n",
        "df=df.fillna(0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeEnuMs3vmTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contamination = 0.05\n",
        "class GMM(GaussianMixture):\n",
        "  def __init__(self, n_components, covariance_type, random_state):\n",
        "    super().__init__(n_components=n_components , covariance_type=covariance_type, random_state=random_state)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    super().fit(X, y)\n",
        "    self.prob = super().score_samples(X)\n",
        "    self.c = percentile(self.prob, 100 * contamination)\n",
        "\n",
        "  def predict(self, X):\n",
        "    scores = []\n",
        "    proba=super().score_samples(X)\n",
        "    \n",
        "    scores =(proba <= self.c).astype('int').ravel()\n",
        "\n",
        "    return scores"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfHT1mLkKSv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the dataset\n",
        "X_train=df.iloc[:1722, 1:].values\n",
        "X_train_val=pd.concat([pd.DataFrame(X_train), pd.DataFrame(np.zeros(len(X_train)))], axis=1)\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33YOEew4Js1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "113b7365-30e8-45b5-d584-b69831fdc549"
      },
      "source": [
        "# Perform k-fold cross validation\n",
        "metric=[]\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def objective(params):\n",
        "  n_components, covariance_type = int(params['n_components']), str(params['covariance_type'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  GMM(n_components= n_components, covariance_type=covariance_type, random_state=0,)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data, np.zeros(X_train.shape[0]))\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "space= {'n_components':hp.quniform('n_components', 2, 6,1), \n",
        "        'covariance_type': hp.choice('covariance_type', [ 'tied' ,'full', 'spherical','diag'])}\n",
        "        \n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "gmm_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print(gmm_best)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:06<00:00,  7.26it/s, best loss: 0.17753623188405787]\n",
            "{'covariance_type': 3, 'n_components': 4.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLr6TihuirEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "62497889-5457-45b9-99ce-4622a5a3a0b1"
      },
      "source": [
        "#  Hyperparameters for HBOS algorithm\n",
        "\n",
        "metric=[]\n",
        "\n",
        "def objective(params):\n",
        "  n_bins, alpha= int(params['n_bins']), float(params['alpha'])#, float(params['tol'])\n",
        "  #n_components,  covariance_type= int(params['n_components']), str(params['covariance_type']) #int(params['n_clusters']) , int(params['beta']), (params['alpha'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model = HBOS(contamination=0.05, n_bins= n_bins, alpha=alpha)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "space= {'n_bins': hp.quniform('n_bins', 10, 20, 1), \n",
        "   'alpha': hp.quniform('alpha', 0.2, 0.9, 0.1)}\n",
        "   \n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "hbos_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best HBOS hyperparemeters:', hbos_best)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:01<00:00, 33.64it/s, best loss: 0.42028985507246375]\n",
            "Best HBOS hyperparemeters: {'alpha': 0.7000000000000001, 'n_bins': 15.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNVsuxHrp4Tt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b9e0b3f6-35df-4dda-93db-2b93c39a0b60"
      },
      "source": [
        "# Hyperparameters for OCSVM algorithm\n",
        "\n",
        "metric=[]\n",
        "\n",
        "def objective(params):\n",
        "  nu, degree, gamma=  float(params['nu']) , int(params['degree']),float(params['gamma'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  OCSVM(contamination=0.05, kernel='rbf' , nu= nu, degree=degree, gamma=gamma)\n",
        "\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "\n",
        "space= {'nu': hp.quniform('nu', 0.1, 0.9, 0.1),\n",
        "        'degree': hp.quniform('degree', 2, 10, 1) , \n",
        "        'gamma' : hp.quniform('gamma', 1, 10, 1)}\n",
        "\n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        " \n",
        "# Print the hyperparameters that minimize the loss function\n",
        "ocsvm_best= fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best OCSVM hyperparemeters', ocsvm_best)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:07<00:00,  1.35s/it, best loss: 0.15652173913043477]\n",
            "Best OCSVM hyperparemeters {'degree': 10.0, 'gamma': 7.0, 'nu': 0.5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFJs4kkQp5CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f50095fc-e280-4f83-f0bf-c85be0fa0001"
      },
      "source": [
        "# Hyperparameters for Feature Bagging algorithm\n",
        "\n",
        "metric=[]\n",
        "def objective(params):\n",
        "  n_neighbors = int(params['n_neighbors'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model = FeatureBagging(LOF(n_neighbors=n_neighbors), contamination=0.05, random_state=0)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "space= {'n_neighbors':hp.quniform('n_neighbors', 1, 15, 1)}\n",
        "\n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "featbag_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best Feature Bagging hyperparemeters:' , featbag_best)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [03:02<00:00,  3.64s/it, best loss: 0.08695652173913049]\n",
            "Best Feature Bagging hyperparemeters: {'n_neighbors': 8.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-WE61A7p5wG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4a7d4e7c-5db1-448a-9f32-eb35ed857b1b"
      },
      "source": [
        "# Hyperparameters for kNN algorithm\n",
        "\n",
        "metric=[]\n",
        "def objective(params):\n",
        "  method, n_neighbors = str(params['methods']) , int(params['n_neighbors'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  model =  KNN(contamination=0.05, method=method, n_neighbors=n_neighbors) \n",
        "  LOF(n_neighbors=n_neighbors, contamination=0.05)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "\n",
        "space= { 'methods': hp.choice('methods', ['largest' ,'median', 'mean']),\n",
        "    'n_neighbors':hp.quniform('n_neighbors', 5, 30, 1)}\n",
        "\n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "knn_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best kNN hyperparemeters:', knn_best)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:50<00:00,  1.00s/it, best loss: 0.10434782608695647]\n",
            "Best kNN hyperparemeters: {'methods': 2, 'n_neighbors': 17.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t72pCydLp6YF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d5aceffa-80bf-4331-bee1-22ad08e8f9df"
      },
      "source": [
        "# Hyperparameters for CBLOF algorithm\n",
        "\n",
        "metric=[]\n",
        "def objective(params):\n",
        "  n_clusters,beta = int(params['n_clusters']),  int(params['beta'])  #alpha, float(params['alpha']),\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model = CBLOF(contamination=0.05, n_clusters=n_clusters,  beta=beta, random_state=0) #alpha=alpha,\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "space= {'n_clusters':hp.quniform('n_clusters', 8, 16, 2),\n",
        "    'beta': hp.quniform('beta', 2, 10 ,2)  }\n",
        "    \n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "cblof_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate)\n",
        "print('Best CBLOF hyperparemeters:', cblof_best)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:22<00:00,  2.19it/s, best loss: 0.230434782608696]\n",
            "Best CBLOF hyperparemeters: {'beta': 4.0, 'n_clusters': 10.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir7FB0YkwoXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "891e0bf4-2af3-4cea-d0aa-6619fc2058ef"
      },
      "source": [
        "# Hyperparameters for LOF algorithm\n",
        "\n",
        "metric=[]\n",
        "def objective(params):\n",
        "  n_neighbors = int(params['n_neighbors'])\n",
        "  #n_bins, alpha, tol = int(params['n_bins']), float(params['alpha']), float(params['tol'])\n",
        "  #n_components,  covariance_type= int(params['n_components']), str(params['covariance_type']) #int(params['n_clusters']) , int(params['beta']), (params['alpha'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  LOF(n_neighbors=n_neighbors, contamination=0.05)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "space= {'n_neighbors':hp.quniform('n_neighbors', 5, 25, 1)}\n",
        "\n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "lof_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate )\n",
        "print('Best LOF hyperparemeters:',lof_best)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:29<00:00,  1.68it/s, best loss: 0.061352657004830946]\n",
            "Best LOF hyperparemeters: {'n_neighbors': 10.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNGbX1i8z9Wd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "796c33c0-4828-4a3f-814f-a8cf08c5bae9"
      },
      "source": [
        "# Hyperparameters for Iforest algorithm\n",
        "\n",
        "metric=[]\n",
        "def objective(params):\n",
        "  n_estimators = int(params['n_estimators'])\n",
        "  kf = KFold(n_splits = 5)\n",
        "  kf.get_n_splits(X_train_val)\n",
        "  \n",
        "  model =  IForest(behaviour=\"new\", bootstrap=False, contamination=0.05, n_estimators=n_estimators,  max_features=1.0, max_samples=1000)\n",
        "  for train, test in kf.split(X_train_val):\n",
        "    train_data = np.array(X_train_val)[train]\n",
        "    train_label = train_data[:,-1]\n",
        "    test_data = np.array(X_train_val)[test]\n",
        "    test_label = test_data[:, -1]\n",
        "    train_data = np.vstack([train_data, np.hstack([train_data[:,24:], train_data[:,:24]])])\n",
        "    train_label = np.concatenate([train_label, train_label])\n",
        "    model.fit(train_data)\n",
        "    pred_train = model.predict(train_data)\n",
        "    pred_test = model.predict(test_data)\n",
        "    metric.append(metrics.accuracy_score(pred_test, test_label))\n",
        "    best_score = Average(metric)\n",
        "    loss=1-best_score\n",
        "    return {'loss':loss, 'status':STATUS_OK}\n",
        "\n",
        "space= {'n_estimators':hp.quniform('n_estimators', 100, 500, 10)}\n",
        "\n",
        "# Hyperopt settings\n",
        "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
        "learning_rate={'learning_rate':hp.loguniform('learning_rate', np.log(0.001), np.log(0.01))}\n",
        "tpe_algorithm =tpe.suggest\n",
        "bayes_trials =Trials()\n",
        "MAX_EVALS=50\n",
        "rstate = np.random.RandomState(0)\n",
        "\n",
        "# Print the hyperparameters that minimize the loss function\n",
        "ifor_best = fmin(objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS, trials= bayes_trials, rstate = rstate )\n",
        "print('Best Iforest hyperparemeters:', ifor_best)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:47<00:00,  2.14s/it, best loss: 0.22246376811594193]\n",
            "Best Iforest hyperparemeters: {'n_estimators': 310.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy3AGK_CX8ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers = {\n",
        "    'Gaussiann Mixture Model (GMM)': GMM(n_components= gmm_best['n_components'], covariance_type=gmm_best['covariance_type'], random_state=0), \n",
        "    'K Nearest Neighbors (KNN)': KNN(contamination=0.05, method=knn_best['methods'], n_neighbors= knn_best['n_neighbors'] ),\n",
        "    'Histogram-base Outlier Detection (HBOS)': HBOS(contamination=0.05, n_bins=hbos_best['n_bins'], alpha=hbos_best['alpha']),\n",
        "    'Feature Bagging':\n",
        "     FeatureBagging(LOF(n_neighbors=featbag_best['n_neighbors']), contamination=0.05),\n",
        "    'Isolation Forest': IForest(behaviour=\"new\", bootstrap=False, contamination=0.05, n_estimators=ifor_best['n_estimators'],  max_features=1.0, max_samples=1000), \n",
        "    'One class SVM (OCSVM)': OCSVM(contamination=0.05, kernel='rbf' , nu=ocsvm_best['nu'] , degree=ocsvm_best['degree'], gamma=ocsvm_best['gamma']),\n",
        "    'Local Outlier Factor (LOF)':\n",
        "       LOF(n_neighbors=lof_best['n_neighbors'], contamination=0.05),\n",
        "     'CBLOF':   CBLOF(contamination=0.05,  beta=cblof_best['beta'], n_clusters=cblof_best['n_clusters'])\n",
        "}"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-0WdgJFNjkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "355b2190-c6ea-45e6-9bfb-751d3f30156f"
      },
      "source": [
        "classifiers"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CBLOF': CBLOF(alpha=0.9, beta=4.0, check_estimator=False, clustering_estimator=None,\n",
              "    contamination=0.05, n_clusters=10.0, n_jobs=1, random_state=None,\n",
              "    use_weights=False),\n",
              " 'Feature Bagging': FeatureBagging(base_estimator=LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
              "   metric_params=None, n_jobs=1, n_neighbors=8.0, p=2),\n",
              "         bootstrap_features=False, check_detector=True,\n",
              "         check_estimator=False, combination='average', contamination=0.05,\n",
              "         estimator_params={}, max_features=1.0, n_estimators=10, n_jobs=1,\n",
              "         random_state=None, verbose=0),\n",
              " 'Gaussiann Mixture Model (GMM)': GMM(covariance_type=3, n_components=4.0, random_state=0),\n",
              " 'Histogram-base Outlier Detection (HBOS)': HBOS(alpha=0.7000000000000001, contamination=0.05, n_bins=15.0, tol=0.5),\n",
              " 'Isolation Forest': IForest(behaviour='new', bootstrap=False, contamination=0.05,\n",
              "     max_features=1.0, max_samples=1000, n_estimators=310.0, n_jobs=1,\n",
              "     random_state=None, verbose=0),\n",
              " 'K Nearest Neighbors (KNN)': KNN(algorithm='auto', contamination=0.05, leaf_size=30, method=2,\n",
              "   metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=17.0, p=2,\n",
              "   radius=1.0),\n",
              " 'Local Outlier Factor (LOF)': LOF(algorithm='auto', contamination=0.05, leaf_size=30, metric='minkowski',\n",
              "   metric_params=None, n_jobs=1, n_neighbors=10.0, p=2),\n",
              " 'One class SVM (OCSVM)': OCSVM(cache_size=200, coef0=0.0, contamination=0.05, degree=10.0, gamma=7.0,\n",
              "    kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
              "    verbose=False)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    }
  ]
}